{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import tensorboard as tb\n",
    "import os\n",
    "# 数値演算\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# 音声波形の読み込み\n",
    "from scipy.io import wavfile\n",
    "# 音声分析\n",
    "# import pyworld\n",
    "# 音声分析、可視化\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語彙の定義\n",
    "characters = \"abcdefghijklmnopqrstuvwxyz!'(),-.:;? \"\n",
    "# その他特殊記号\n",
    "extra_symbols = [\n",
    "    \"^\",  # 文の先頭を表す特殊記号 <SOS>\n",
    "    \"$\",  # 文の末尾を表す特殊記号 <EOS>\n",
    "]\n",
    "_pad = \"~\"\n",
    "\n",
    "# NOTE: パディングを 0 番目に配置\n",
    "symbols = [_pad] + extra_symbols + list(characters)\n",
    "\n",
    "# 文字列⇔数値の相互変換のための辞書\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,s in enumerate(symbols):\n",
    "    print(f\"i = {i}\")\n",
    "    print(f\"s = {s}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text):\n",
    "    # 簡易のため、大文字と小文字を区別せず、全ての大文字を小文字に変換\n",
    "    text = text.lower()\n",
    "\n",
    "    # <SOS>\n",
    "    seq = [_symbol_to_id[\"^\"]]\n",
    "\n",
    "    # 本文\n",
    "    seq += [_symbol_to_id[s] for s in text]\n",
    "\n",
    "    # <EOS>\n",
    "    seq.append(_symbol_to_id[\"$\"])\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [_id_to_symbol[s] for s in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = text_to_sequence(\"Hello!\")\n",
    "print(f\"文字列から数値列への変換: {seq}\")\n",
    "print(f\"数値列から文字列への逆変換: {sequence_to_text(seq)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestEncoder(nn.Module):\n",
    "    def __init__(self, num_vocab=40, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_vocab, embed_dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, seqs):\n",
    "        return self.embed(seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimplestEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_input():\n",
    "    # バッチサイズに 2 を想定して、適当な文字列を作成\n",
    "    seqs = [\n",
    "        text_to_sequence(\"What is your favorite language?\"),\n",
    "        # text_to_sequence(\"Hello world.\"),\n",
    "    ]\n",
    "    in_lens = torch.tensor([len(x) for x in seqs], dtype=torch.long)\n",
    "    max_len = max(len(x) for x in seqs)\n",
    "    # seqs = torch.stack([torch.from_numpy(pad_1d(seq, max_len)) for seq in seqs])\n",
    "    seqs = torch.tensor(seqs)\n",
    "\n",
    "    return seqs, in_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SimplestEncoder(num_vocab=40, embed_dim=256)\n",
    "seqs, in_lens = get_dummy_input()\n",
    "encoder_outs = encoder(seqs)\n",
    "print(f\"入力のサイズ: {tuple(seqs.shape)}\")\n",
    "print(f\"出力のサイズ: {tuple(encoder_outs.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(10, 2)\n",
    "input = torch.tensor([[1,2,4,5], [4,3,2,9]]).long()\n",
    "print(embedding(input))\n",
    "print(embedding(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = text_to_sequence(\"What is your favorite language?\")\n",
    "seq = torch.tensor(seq)\n",
    "encoder = SimplestEncoder(num_vocab=40, embed_dim=256)\n",
    "embed = encoder(seq)\n",
    "print(embed)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vocab=40,\n",
    "        embed_dim=256,\n",
    "        conv_layers=3,\n",
    "        conv_channels=256,\n",
    "        conv_kernel_size=5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 文字埋め込み\n",
    "        self.embed = nn.Embedding(num_vocab, embed_dim, padding_idx=0)\n",
    "\n",
    "        # 1次元畳み込みの重ね合わせ：局所的な依存関係のモデル化\n",
    "        self.convs = nn.ModuleList()\n",
    "        for layer in range(conv_layers):\n",
    "            in_channels = embed_dim if layer == 0 else conv_channels\n",
    "            self.convs += [\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    conv_channels,\n",
    "                    conv_kernel_size,\n",
    "                    padding=(conv_kernel_size - 1) // 2,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(conv_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            ]\n",
    "        self.convs = nn.Sequential(*self.convs)\n",
    "\n",
    "    def forward(self, seqs):\n",
    "        emb = self.embed(seqs)\n",
    "        # 1 次元畳み込みと embedding では、入力のサイズが異なるので注意\n",
    "        out = self.convs(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, in_lens = get_dummy_input()\n",
    "\n",
    "num_vocab = 40\n",
    "embed_dim = 256\n",
    "conv_layers = 3\n",
    "conv_channels = 256\n",
    "conv_kernel_size = 5\n",
    "\n",
    "embed = nn.Embedding(num_vocab, embed_dim, padding_idx=0)\n",
    "embed_out = embed(seqs)\n",
    "print(f\"embed = {embed_out.shape}\")\n",
    "\n",
    "convs = nn.ModuleList()\n",
    "for layer in range(conv_layers):\n",
    "    in_channels = embed_dim if layer == 0 else conv_channels\n",
    "    convs += [\n",
    "        nn.Conv1d(\n",
    "            in_channels,\n",
    "            conv_channels,\n",
    "            conv_kernel_size,\n",
    "            padding=(conv_kernel_size - 1) // 2,\n",
    "            bias=False,\n",
    "        ),\n",
    "        nn.BatchNorm1d(conv_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5)\n",
    "    ]\n",
    "convs = nn.Sequential(*convs)   \n",
    "\n",
    "# print(convs)\n",
    "out = convs(embed_out.transpose(1,2))\n",
    "print(f\"out = {out.shape}\")\n",
    "out = out.transpose(1,2)\n",
    "print(f\"out_T = {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConvEncoder(num_vocab=40, embed_dim=256)\n",
    "seqs, in_lens = get_dummy_input()\n",
    "encoder_outs = encoder(seqs)\n",
    "print(f\"入力のサイズ: {tuple(seqs.shape)}\")\n",
    "print(f\"出力のサイズ: {tuple(encoder_outs.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# エンコーダ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(ConvEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vocab=40,\n",
    "        embed_dim=512,\n",
    "        hidden_dim=512,\n",
    "        conv_layers=5,\n",
    "        conv_channels=512,\n",
    "        conv_kernel_size=7,\n",
    "    ):\n",
    "        # ConvEncoderの引数を継承した後に，Encoderで初期化される。Encoderの引数の値を使用して構築される。\n",
    "        super().__init__(\n",
    "            num_vocab, embed_dim, conv_layers, conv_channels, conv_kernel_size\n",
    "        )\n",
    "        # 双方向 LSTM による長期依存関係のモデル化\n",
    "        self.blstm = nn.LSTM(\n",
    "            conv_channels, hidden_dim // 2, 1, batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, seqs, in_lens):\n",
    "        emb = self.embed(seqs)\n",
    "        # 1 次元畳み込みと embedding では、入力のサイズ が異なるので注意\n",
    "        out = self.convs(emb.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        # 双方向 LSTM の計算\n",
    "        out = pack_padded_sequence(out, in_lens, batch_first=True)\n",
    "        out, _ = self.blstm(out)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = ConvEncoder(num_vocab=40, embed_dim=256)\n",
    "encoder = Encoder(num_vocab=40, embed_dim=256)\n",
    "# 継承前の元のやつ\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 継承した後。畳み込み層の数やカーネルサイズが更新されている。\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_vocab=40, embed_dim=256)\n",
    "seqs, in_lens = get_dummy_input()\n",
    "in_lens, indices = torch.sort(in_lens, dim=0, descending=True)\n",
    "seqs = seqs[indices]\n",
    "\n",
    "encoder_outs = encoder(seqs, in_lens)\n",
    "print(f\"入力のサイズ: {tuple(seqs.shape)}\")\n",
    "print(f\"出力のサイズ: {tuple(encoder_outs.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# 書籍中の数式に沿って、わかりやすさを重視した実装\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim=512, decoder_dim=1024, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.V = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.W = nn.Linear(decoder_dim, hidden_dim, bias=False)\n",
    "        # NOTE: 本書の数式通りに実装するなら bias=False ですが、実用上は bias=True としても問題ありません\n",
    "        self.w = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_state, mask=None):\n",
    "        # 式 (9.11) の計算\n",
    "        erg = self.w(\n",
    "            torch.tanh(self.W(decoder_state).unsqueeze(1) + self.V(encoder_outs))\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            erg.masked_fill_(mask, -float(\"inf\"))\n",
    "\n",
    "        attention_weights = F.softmax(erg, dim=1)\n",
    "\n",
    "        # エンコーダ出力の長さ方向に対して重み付き和を取ります\n",
    "        attention_context = torch.sum(\n",
    "            encoder_outs * attention_weights.unsqueeze(-1), dim=1\n",
    "        )\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outs = torch.rand(2, 33, 512)\n",
    "decoder_input = torch.rand(2, 1024)\n",
    "attention = BahdanauAttention()\n",
    "attention_context, attention_weights = attention(encoder_outs, decoder_input)\n",
    "\n",
    "print(f\"エンコーダの出力のサイズ: {tuple(encoder_outs.shape)}\")\n",
    "print(f\"デコーダの隠れ状態のサイズ: {tuple(decoder_input.shape)}\")\n",
    "print(f\"コンテキストベクトルのサイズ: {tuple(attention_context.shape)}\")\n",
    "print(f\"アテンション重みのサイズ: {tuple(attention_weights.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outs = torch.rand(2, 33, 512)\n",
    "decoder_input = torch.rand(2, 1024)\n",
    "\n",
    "encoder_dim=512\n",
    "decoder_dim=1024\n",
    "hidden_dim=128\n",
    "\n",
    "V = nn.Linear(encoder_dim, hidden_dim)\n",
    "W = nn.Linear(decoder_dim, hidden_dim, bias=False)\n",
    "w = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "erg = w(\n",
    "    torch.tanh(W(decoder_input).unsqueeze(1) + V(encoder_outs))\n",
    ")\n",
    "erg_sq = erg.squeeze(-1)\n",
    "\n",
    "attention_weights = F.softmax(erg_sq, dim=1)\n",
    "\n",
    "attention_context = torch.sum(\n",
    "    encoder_outs * attention_weights.unsqueeze(-1), dim=1\n",
    ")\n",
    "\n",
    "test = encoder_outs * attention_weights.unsqueeze(-1)\n",
    "\n",
    "print(f\"W(decoder_input).unsqueeze(1).shape = {W(decoder_input).unsqueeze(1).shape}\")\n",
    "print(f\"V(encoder_outs).shape = {V(encoder_outs).shape}\")\n",
    "print(f\"erg.shape = {erg.shape}\")\n",
    "print(f\"erg_sq.shape = {erg_sq.shape}\")\n",
    "print(f\"attention_weights.shape = {attention_weights.shape}\")\n",
    "print(f\"encoder_outs = {encoder_outs.shape}\")\n",
    "print(f\"attention_weights = {attention_weights.shape}\")\n",
    "print(f\"attention_weights.unsq = {attention_weights.unsqueeze(-1).shape}\")\n",
    "print(f\"pre_sum = {test.shape}\")\n",
    "print(attention_context.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,5,1)\n",
    "y = torch.rand(2,5,33)\n",
    "z = x * y\n",
    "# sum = torch.sum(x * y, dim=1)\n",
    "a = torch.tensor([[1,2], [3,4]])\n",
    "b = torch.tensor([[1,2], [3,4]])\n",
    "c = a * b\n",
    "print(a)\n",
    "print(c)\n",
    "\n",
    "# unsqueezeでサイズ1の次元を追加することで，アダマール積が求められる。\n",
    "# 本当はサイズが合ってないと要素積はできないけど，行列×定数ができるのと同じルールで計算できちゃう。\n",
    "a = torch.arange(0, 20).view(2,2,-1)\n",
    "b = torch.arange(0, 4).view(2,2).unsqueeze(-1)\n",
    "print(\"\")\n",
    "print(f\"a.shape = {a.shape}\")\n",
    "print(f\"b.shape = {b.shape}\")\n",
    "print(a)\n",
    "print(b)\n",
    "c = a * b\n",
    "print(f\"c.shape = {c.shape}\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(4).view(2,-1)\n",
    "# b = torch.ones(4).view(2,-1) * 4\n",
    "b = 4\n",
    "c = a * b\n",
    "print(c)\n",
    "d = a + b\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeezeは次元の中でサイズ1のところを消去する。引数で次元を選択。\n",
    "x = torch.zeros(2,3,1,4,1)\n",
    "print(x.shape)\n",
    "print(x.squeeze().shape)\n",
    "print(x.squeeze(-1).shape)\n",
    "print(x.unsqueeze(-1).shape)\n",
    "print(x.squeeze().unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pad_mask(lengths, maxlen=None):\n",
    "    \"\"\"Make mask for padding frames\n",
    "\n",
    "    Args:\n",
    "        lengths (list): list of lengths\n",
    "        maxlen (int, optional): maximum length. If None, use max value of lengths.\n",
    "\n",
    "    Returns:\n",
    "        torch.ByteTensor: mask\n",
    "    \"\"\"\n",
    "    if not isinstance(lengths, list):\n",
    "        lengths = lengths.tolist()\n",
    "    bs = int(len(lengths))\n",
    "    if maxlen is None:\n",
    "        maxlen = int(max(lengths))\n",
    "\n",
    "    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n",
    "    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n",
    "    mask = seq_range_expand >= seq_length_expand\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意機構（内容依存と位置依存のハイブリッド）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationSensitiveAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_dim=512,\n",
    "        decoder_dim=1024,\n",
    "        hidden_dim=128,\n",
    "        conv_channels=32,\n",
    "        conv_kernel_size=31,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.V = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.W = nn.Linear(decoder_dim, hidden_dim, bias=False)\n",
    "        self.U = nn.Linear(conv_channels, hidden_dim, bias=False)\n",
    "        self.F = nn.Conv1d(\n",
    "            1,\n",
    "            conv_channels,\n",
    "            conv_kernel_size,\n",
    "            padding=(conv_kernel_size - 1) // 2,\n",
    "            bias=False,\n",
    "        )\n",
    "        # NOTE: 本書の数式通りに実装するなら bias=False ですが、実用上は bias=True としても問題ありません\n",
    "        self.w = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outs, src_lens, decoder_state, att_prev, mask=None):\n",
    "        # アテンション重みを一様分布で初期化\n",
    "        if att_prev is None:\n",
    "            att_prev = 1.0 - make_pad_mask(src_lens).to(\n",
    "                device=decoder_state.device, dtype=decoder_state.dtype\n",
    "            )\n",
    "            att_prev = att_prev / src_lens.unsqueeze(-1).to(encoder_outs.device)\n",
    "\n",
    "        # (B x T_enc) -> (B x 1 x T_enc) -> (B x conv_channels x T_enc) ->\n",
    "        # (B x T_enc x conv_channels)\n",
    "        f = self.F(att_prev.unsqueeze(1)).transpose(1, 2)\n",
    "\n",
    "        # 式 (9.13) の計算\n",
    "        erg = self.w(\n",
    "            torch.tanh(\n",
    "                self.W(decoder_state).unsqueeze(1) + self.V(encoder_outs) + self.U(f)\n",
    "            )\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            erg.masked_fill_(mask, -float(\"inf\"))\n",
    "\n",
    "        attention_weights = F.softmax(erg, dim=1)\n",
    "\n",
    "        # エンコーダ出力の長さ方向に対して重み付き和を取ります\n",
    "        attention_context = torch.sum(\n",
    "            encoder_outs * attention_weights.unsqueeze(-1), dim=1\n",
    "        )\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "エンコーダの出力のサイズ: (2, 33, 512)\n",
      "デコーダの隠れ状態のサイズ: (1, 1024)\n",
      "コンテキストベクトルのサイズ: (2, 512)\n",
      "アテンション重みのサイズ: (2, 33)\n"
     ]
    }
   ],
   "source": [
    "from ttslearn.util import make_pad_mask\n",
    "\n",
    "mask =  make_pad_mask(in_lens).to(encoder_outs.device)\n",
    "attention = LocationSensitiveAttention()\n",
    "\n",
    "decoder_input = torch.ones(len(seqs), 1024)\n",
    "\n",
    "attention_context, attention_weights = attention(encoder_outs, in_lens, decoder_input, None, mask)\n",
    "\n",
    "print(f\"エンコーダの出力のサイズ: {tuple(encoder_outs.shape)}\")\n",
    "print(f\"デコーダの隠れ状態のサイズ: {tuple(decoder_input.shape)}\")\n",
    "print(f\"コンテキストベクトルのサイズ: {tuple(attention_context.shape)}\")\n",
    "print(f\"アテンション重みのサイズ: {tuple(attention_weights.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, layers=2, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        prenet = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            prenet += [\n",
    "                nn.Linear(in_dim if layer == 0 else hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        self.prenet = nn.Sequential(*prenet)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.prenet:\n",
    "            # 学習時、推論時の両方で Dropout を適用します\n",
    "            x = F.dropout(layer(x), self.dropout, training=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqs = torch.Size([1, 33])\n",
      "デコーダの入力のサイズ: (1, 80)\n",
      "Pre-Net の出力のサイズ: (1, 256)\n"
     ]
    }
   ],
   "source": [
    "decoder_input = torch.ones(len(seqs), 80)\n",
    "\n",
    "prenet = Prenet(decoder_input.shape[1])\n",
    "out = prenet(decoder_input)\n",
    "print(f\"seqs = {seqs.shape}\")\n",
    "print(f\"デコーダの入力のサイズ: {tuple(decoder_input.shape)}\")\n",
    "print(f\"Pre-Net の出力のサイズ: {tuple(out.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZoneOutCell(nn.Module):\n",
    "    def __init__(self, cell, zoneout=0.1):\n",
    "        super().__init__()\n",
    "        self.cell = cell\n",
    "        self.hidden_size = cell.hidden_size\n",
    "        self.zoneout = zoneout\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        next_hidden = self.cell(inputs, hidden)\n",
    "        next_hidden = self._zoneout(hidden, next_hidden, self.zoneout)\n",
    "        return next_hidden\n",
    "\n",
    "    def _zoneout(self, h, next_h, prob):\n",
    "        h_0, c_0 = h\n",
    "        h_1, c_1 = next_h\n",
    "        h_1 = self._apply_zoneout(h_0, h_1, prob)\n",
    "        c_1 = self._apply_zoneout(c_0, c_1, prob)\n",
    "        return h_1, c_1\n",
    "\n",
    "    def _apply_zoneout(self, h, next_h, prob):\n",
    "        if self.training:\n",
    "            mask = h.new(*h.size()).bernoulli_(prob)\n",
    "            return mask * h + (1 - mask) * next_h\n",
    "        else:\n",
    "            return prob * h + (1 - prob) * next_h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# デコーダ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ttslearn.tacotron.decoder import ZoneOutCell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_hidden_dim=512,\n",
    "        out_dim=80,\n",
    "        layers=2,\n",
    "        hidden_dim=1024,\n",
    "        prenet_layers=2,\n",
    "        prenet_hidden_dim=256,\n",
    "        prenet_dropout=0.5,\n",
    "        zoneout=0.1,\n",
    "        reduction_factor=1,\n",
    "        attention_hidden_dim=128,\n",
    "        attention_conv_channels=32,\n",
    "        attention_conv_kernel_size=31,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        # 注意機構\n",
    "        self.attention = LocationSensitiveAttention(\n",
    "            encoder_hidden_dim,\n",
    "            hidden_dim,\n",
    "            attention_hidden_dim,\n",
    "            attention_conv_channels,\n",
    "            attention_conv_kernel_size,\n",
    "        )\n",
    "        self.reduction_factor = reduction_factor\n",
    "\n",
    "        # Prenet\n",
    "        self.prenet = Prenet(out_dim, prenet_layers, prenet_hidden_dim, prenet_dropout)\n",
    "\n",
    "        # 片方向LSTM\n",
    "        self.lstm = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            lstm = nn.LSTMCell(\n",
    "                encoder_hidden_dim + prenet_hidden_dim if layer == 0 else hidden_dim,\n",
    "                hidden_dim,\n",
    "            )\n",
    "            lstm = ZoneOutCell(lstm, zoneout)\n",
    "            self.lstm += [lstm]\n",
    "\n",
    "        # 出力への projection 層\n",
    "        proj_in_dim = encoder_hidden_dim + hidden_dim\n",
    "        self.feat_out = nn.Linear(proj_in_dim, out_dim * reduction_factor, bias=False)\n",
    "        self.prob_out = nn.Linear(proj_in_dim, reduction_factor)\n",
    "\n",
    "    def _zero_state(self, hs):\n",
    "        init_hs = hs.new_zeros(hs.size(0), self.lstm[0].hidden_size)\n",
    "        return init_hs\n",
    "\n",
    "    def forward(self, encoder_outs, in_lens, decoder_targets=None):\n",
    "        is_inference = decoder_targets is None\n",
    "\n",
    "        # Reduction factor に基づくフレーム数の調整\n",
    "        # (B, Lmax, out_dim) ->  (B, Lmax/r, out_dim)\n",
    "        if self.reduction_factor > 1 and not is_inference:\n",
    "            decoder_targets = decoder_targets[\n",
    "                :, self.reduction_factor - 1 :: self.reduction_factor\n",
    "            ]\n",
    "\n",
    "        # デコーダの系列長を保持\n",
    "        # 推論時は、エンコーダの系列長から経験的に上限を定める\n",
    "        if is_inference:\n",
    "            max_decoder_time_steps = int(encoder_outs.shape[1] * 10.0)\n",
    "        else:\n",
    "            max_decoder_time_steps = decoder_targets.shape[1]\n",
    "\n",
    "        # ゼロパディングされた部分に対するマスク\n",
    "        mask = make_pad_mask(in_lens).to(encoder_outs.device)\n",
    "\n",
    "        # LSTM の状態をゼロで初期化\n",
    "        h_list, c_list = [], []\n",
    "        for _ in range(len(self.lstm)):\n",
    "            h_list.append(self._zero_state(encoder_outs))\n",
    "            c_list.append(self._zero_state(encoder_outs))\n",
    "\n",
    "        # デコーダの最初の入力\n",
    "        go_frame = encoder_outs.new_zeros(encoder_outs.size(0), self.out_dim)\n",
    "        prev_out = go_frame\n",
    "\n",
    "        # 1つ前の時刻のアテンション重み\n",
    "        prev_att_w = None\n",
    "\n",
    "        # メインループ\n",
    "        outs, logits, att_ws = [], [], []\n",
    "        t = 0\n",
    "        while True:\n",
    "            # コンテキストベクトル、アテンション重みの計算\n",
    "            # h_list\n",
    "            att_c, att_w = self.attention(\n",
    "                encoder_outs, in_lens, h_list[0], prev_att_w, mask\n",
    "            )\n",
    "\n",
    "            # Pre-Net\n",
    "            prenet_out = self.prenet(prev_out)\n",
    "\n",
    "            # LSTM\n",
    "            # hが出力，cがセルの状態\n",
    "            # xsはコンテキストベクトルとprenetの出力の結合\n",
    "            xs = torch.cat([att_c, prenet_out], dim=1)\n",
    "            h_list[0], c_list[0] = self.lstm[0](xs, (h_list[0], c_list[0]))\n",
    "            for i in range(1, len(self.lstm)):\n",
    "                h_list[i], c_list[i] = self.lstm[i](\n",
    "                    h_list[i - 1], (h_list[i], c_list[i])\n",
    "                )\n",
    "            # 出力の計算\n",
    "            hcs = torch.cat([h_list[-1], att_c], dim=1)\n",
    "            outs.append(self.feat_out(hcs).view(encoder_outs.size(0), self.out_dim, -1))\n",
    "            logits.append(self.prob_out(hcs))\n",
    "            att_ws.append(att_w)\n",
    "\n",
    "            # 次の時刻のデコーダの入力を更新\n",
    "            if is_inference:\n",
    "                prev_out = outs[-1][:, :, -1]  # (1, out_dim)\n",
    "            else:\n",
    "                # Teacher forcing\n",
    "                prev_out = decoder_targets[:, t, :]\n",
    "\n",
    "            # 累積アテンション重み\n",
    "            prev_att_w = att_w if prev_att_w is None else prev_att_w + att_w\n",
    "\n",
    "            t += 1\n",
    "            # 停止条件のチェック\n",
    "            if t >= max_decoder_time_steps:\n",
    "                break\n",
    "            if is_inference and (torch.sigmoid(logits[-1]) >= 0.5).any():\n",
    "                break\n",
    "\n",
    "        # # 各時刻の出力を結合\n",
    "        logits = torch.cat(logits, dim=1)  # (B, Lmax)\n",
    "        outs = torch.cat(outs, dim=2)  # (B, out_dim, Lmax)\n",
    "        att_ws = torch.stack(att_ws, dim=1)  # (B, Lmax, Tmax)\n",
    "\n",
    "        if self.reduction_factor > 1:\n",
    "            outs = outs.view(outs.size(0), self.out_dim, -1)  # (B, out_dim, Lmax)\n",
    "\n",
    "        return outs, logits, att_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_targets = torch.Size([2, 120, 80])\n",
      "デコーダの入力のサイズ: (1, 80)\n",
      "デコーダの出力のサイズ: (2, 80, 120)\n",
      "stop token (logits) のサイズ: (2, 120)\n",
      "アテンション重みのサイズ: (2, 120, 33)\n"
     ]
    }
   ],
   "source": [
    "decoder_targets = torch.ones(encoder_outs.shape[0], 120, 80)\n",
    "print(f\"decoder_targets = {decoder_targets.shape}\")\n",
    "decoder = Decoder(encoder_outs.shape[-1], 80)\n",
    "\n",
    "# Teaccher forcing: decoder_targets (教師データ) を与える\n",
    "with torch.no_grad():\n",
    "    outs, logits, att_ws = decoder(encoder_outs, in_lens, decoder_targets)\n",
    "\n",
    "print(f\"デコーダの入力のサイズ: {tuple(decoder_input.shape)}\")\n",
    "print(f\"デコーダの出力のサイズ: {tuple(outs.shape)}\")\n",
    "print(f\"stop token (logits) のサイズ: {tuple(logits.shape)}\")\n",
    "print(f\"アテンション重みのサイズ: {tuple(att_ws.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デコーダの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_dim = 512\n",
    "out_dim=80\n",
    "layers=2\n",
    "hidden_dim=1024\n",
    "prenet_layers=2\n",
    "prenet_hidden_dim=256\n",
    "prenet_dropout=0.5\n",
    "zoneout=0.1\n",
    "reduction_factor=1\n",
    "attention_hidden_dim=128\n",
    "attention_conv_channels=32\n",
    "attention_conv_kernel_size=31\n",
    "\n",
    "attention = LocationSensitiveAttention(\n",
    "            encoder_hidden_dim,\n",
    "            hidden_dim,\n",
    "            attention_hidden_dim,\n",
    "            attention_conv_channels,\n",
    "            attention_conv_kernel_size,\n",
    "        )\n",
    "\n",
    "prenet = Prenet(out_dim, prenet_layers, prenet_hidden_dim, prenet_dropout) \n",
    "\n",
    "\"\"\"\n",
    "convs = nn.ModuleList()\n",
    "for layer in range(conv_layers):\n",
    "    in_channels = embed_dim if layer == 0 else conv_channels\n",
    "    convs += [\n",
    "        nn.Conv1d(\n",
    "            in_channels,\n",
    "            conv_channels,\n",
    "            conv_kernel_size,\n",
    "            padding=(conv_kernel_size - 1) // 2,\n",
    "            bias=False,\n",
    "        ),\n",
    "        nn.BatchNorm1d(conv_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5)\n",
    "    ]\n",
    "convs = nn.Sequential(*convs)\n",
    "\"\"\"\n",
    "\n",
    "lstm = nn.ModuleList()\n",
    "for layer in range(layers):\n",
    "    lstm_c = nn.LSTMCell(\n",
    "        encoder_hidden_dim + prenet_hidden_dim if layer == 0 else hidden_dim,\n",
    "        hidden_dim,\n",
    "    )\n",
    "    # print(lstm_c)\n",
    "    lstm_c = ZoneOutCell(lstm_c, zoneout)\n",
    "    # print(lstm_c)\n",
    "    lstm_c = [lstm_c]\n",
    "    # print(lstm_c)\n",
    "    lstm += lstm_c\n",
    "    # print(lstm)\n",
    "    # print(\"\")\n",
    "\n",
    "# 出力への projection 層\n",
    "proj_in_dim = encoder_hidden_dim + hidden_dim\n",
    "feat_out = nn.Linear(proj_in_dim, out_dim * reduction_factor, bias=False)\n",
    "prob_out = nn.Linear(proj_in_dim, reduction_factor)\n",
    "\n",
    "def _zero_state(hs):\n",
    "        init_hs = hs.new_zeros(hs.size(0), lstm[0].hidden_size)\n",
    "        return init_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(lstm[0].hidden_size)\n",
    "print(len(lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): ZoneOutCell(\n",
       "    (cell): LSTMCell(768, 1024)\n",
       "  )\n",
       "  (1): ZoneOutCell(\n",
       "    (cell): LSTMCell(1024, 1024)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outs = torch.Size([2, 33, 512])\n",
      "h_list = tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "c_list = 2\n",
      "_zero_state = torch.Size([2, 1024])\n",
      "go_frame = torch.Size([2, 80])\n"
     ]
    }
   ],
   "source": [
    "encoder_outs = encoder_outs\n",
    "in_lens = in_lens\n",
    "decoder_targets = None\n",
    "print(f\"encoder_outs = {encoder_outs.shape}\")\n",
    "\n",
    "# decoder_targets = Noneのとき，is_inference = True\n",
    "is_inference = decoder_targets is None\n",
    "# print(f\"is_inference = {is_inference}\")\n",
    "\n",
    "# Reduction factor に基づくフレーム数の調整\n",
    "# (B, Lmax, out_dim) ->  (B, Lmax/r, out_dim)\n",
    "# is_inference = Trueでないときに実行\n",
    "if reduction_factor > 1 and not is_inference:\n",
    "    decoder_targets = decoder_targets[\n",
    "        :, reduction_factor - 1 :: reduction_factor\n",
    "    ]\n",
    "\n",
    "# デコーダの系列長を保持\n",
    "# 推論時は、エンコーダの系列長から経験的に上限を定める\n",
    "if is_inference:\n",
    "    max_decoder_time_steps = int(encoder_outs.shape[1] * 10.0)\n",
    "else:\n",
    "    max_decoder_time_steps = decoder_targets.shape[1]\n",
    "\n",
    "# ゼロパディングされた部分に対するマスク\n",
    "mask = make_pad_mask(in_lens).to(encoder_outs.device)\n",
    "\n",
    "# LSTM の状態をゼロで初期化\n",
    "# encoder_outs.size[0], lstm[0].hidden_size\n",
    "h_list, c_list = [], []\n",
    "for _ in range(len(lstm)):\n",
    "    h_list.append(_zero_state(encoder_outs))\n",
    "    c_list.append(_zero_state(encoder_outs))\n",
    "print(f\"h_list = {h_list}\")\n",
    "print(f\"c_list = {len(c_list)}\")\n",
    "print(f\"_zero_state = {_zero_state(encoder_outs).shape}\")\n",
    "\n",
    "# デコーダの最初の入力\n",
    "go_frame = encoder_outs.new_zeros(encoder_outs.size(0), out_dim)\n",
    "print(f\"go_frame = {go_frame.shape}\")\n",
    "prev_out = go_frame\n",
    "\n",
    "# 1つ前の時刻のアテンション重み\n",
    "prev_att_w = None\n",
    "\n",
    "# メインループ\n",
    "outs, logits, att_ws = [], [], []\n",
    "t = 0\n",
    "while True:\n",
    "    # コンテキストベクトル、アテンション重みの計算\n",
    "    # h_list\n",
    "    att_c, att_w = attention(\n",
    "        encoder_outs, in_lens, h_list[0], prev_att_w, mask\n",
    "    )\n",
    "\n",
    "    # Pre-Net\n",
    "    prenet_out = prenet(prev_out)\n",
    "\n",
    "    # LSTM\n",
    "    # hが出力，cがセルの状態\n",
    "    # xsはコンテキストベクトルとprenetの出力の結合\n",
    "    xs = torch.cat([att_c, prenet_out], dim=1)\n",
    "    h_list[0], c_list[0] = lstm[0](xs, (h_list[0], c_list[0]))\n",
    "    for i in range(1, len(lstm)):\n",
    "        h_list[i], c_list[i] = lstm[i](\n",
    "            h_list[i - 1], (h_list[i], c_list[i])\n",
    "        )\n",
    "    # 出力の計算\n",
    "    hcs = torch.cat([h_list[-1], att_c], dim=1)\n",
    "    outs.append(feat_out(hcs).view(encoder_outs.size(0), out_dim, -1))\n",
    "    logits.append(prob_out(hcs))\n",
    "    att_ws.append(att_w)\n",
    "\n",
    "    # 次の時刻のデコーダの入力を更新（prenetの入力）\n",
    "    if is_inference:\n",
    "        prev_out = outs[-1][:, :, -1]  # (1, out_dim)\n",
    "    else:\n",
    "        # Teacher forcing\n",
    "        # 学習時は自身の出力を回帰的に参照するのではなく，正しいデータを参照する。\n",
    "        # 学習時（特に初期）は間違った出力を出すことが予想され，それを参照していては学習が進まない。\n",
    "        # そのため，正解データを次の出力のために利用して学習していく。\n",
    "        prev_out = decoder_targets[:, t, :]\n",
    "\n",
    "    # 累積アテンション重み\n",
    "    prev_att_w = att_w if prev_att_w is None else prev_att_w + att_w\n",
    "\n",
    "    t += 1\n",
    "    # 停止条件のチェック\n",
    "    if t >= max_decoder_time_steps:\n",
    "        break\n",
    "    if is_inference and (torch.sigmoid(logits[-1]) >= 0.5).any():\n",
    "        break\n",
    "\n",
    "# 各時刻の出力を結合\n",
    "logits = torch.cat(logits, dim=1)  # (B, Lmax)\n",
    "outs = torch.cat(outs, dim=2)  # (B, out_dim, Lmax)\n",
    "att_ws = torch.stack(att_ws, dim=1)  # (B, Lmax, Tmax)\n",
    "\n",
    "if reduction_factor > 1:\n",
    "    outs = outs.view(outs.size(0), out_dim, -1)  # (B, out_dim, Lmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,100,10)\n",
    "x = torch.arange(3*8*1).view(3,8,1)\n",
    "reduction_factor = 4\n",
    "x_r = x[:, reduction_factor-1 :: reduction_factor, :]\n",
    "x_ = x[:, 1::4, :]\n",
    "print(f\"x = {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_r.shape)\n",
    "print(f\"x_r = {x_r}\")\n",
    "# print(x_.shape)\n",
    "# print(f\"x_ = {x_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "x_0 = torch.cat((x, x, x), 0)\n",
    "x_1 = torch.cat((x, x, x), 1)\n",
    "x_2 = torch.cat((x, x, x), 2)\n",
    "\n",
    "cat_dim = [0, 1, 2]\n",
    "\n",
    "for i in cat_dim:\n",
    "    print(f\"x_{i} = {torch.cat((x, x, x), i).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor((), dtype=float)\n",
    "x.new_zeros((2,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(lstm)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outs = torch.Size([2, 33, 512])\n",
      "h_list = [tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])]\n",
      "c_list = 2\n",
      "_zero_state = torch.Size([2, 1024])\n",
      "go_frame = torch.Size([2, 80])\n",
      "att_c = torch.Size([2, 512])\n",
      "att_w = torch.Size([2, 33])\n",
      "prinet_out = torch.Size([2, 256])\n",
      "xs = torch.Size([2, 768])\n",
      "\n",
      "hcs = torch.Size([2, 1536])\n",
      "feat_out(hcs) = torch.Size([2, 80])\n",
      "feat_out(hcs)_transposed = torch.Size([2, 80, 1])\n",
      "prob_out = torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# ゼロパディングされた部分に対するマスク\n",
    "mask = make_pad_mask(in_lens).to(encoder_outs.device)\n",
    "\n",
    "print(f\"encoder_outs = {encoder_outs.shape}\")\n",
    "\n",
    "# LSTM の状態をゼロで初期化\n",
    "# encoder_outs.size[0], lstm[0].hidden_size\n",
    "h_list, c_list = [], []\n",
    "for _ in range(len(lstm)):\n",
    "    h_list.append(_zero_state(encoder_outs))\n",
    "    c_list.append(_zero_state(encoder_outs))\n",
    "print(f\"h_list = {h_list}\")\n",
    "print(f\"c_list = {len(c_list)}\")\n",
    "print(f\"_zero_state = {_zero_state(encoder_outs).shape}\")\n",
    "\n",
    "# デコーダの最初の入力\n",
    "go_frame = encoder_outs.new_zeros(encoder_outs.size(0), out_dim)\n",
    "print(f\"go_frame = {go_frame.shape}\")\n",
    "prev_out = go_frame\n",
    "\n",
    "# 1つ前の時刻のアテンション重み\n",
    "prev_att_w = None\n",
    "\n",
    "# メインループ\n",
    "outs, logits, att_ws = [], [], []\n",
    "t = 0\n",
    "\n",
    "# コンテキストベクトル、アテンション重みの計算\n",
    "# h_list\n",
    "att_c, att_w = attention(\n",
    "    encoder_outs, in_lens, h_list[0], prev_att_w, mask\n",
    ")\n",
    "print(f\"att_c = {att_c.shape}\")\n",
    "print(f\"att_w = {att_w.shape}\")\n",
    "\n",
    "# Pre-Net\n",
    "prenet_out = prenet(prev_out)\n",
    "print(f\"prinet_out = {prenet_out.shape}\")\n",
    "\n",
    "# LSTM\n",
    "# hが出力，cがセルの状態\n",
    "# xsはコンテキストベクトルとprenetの出力の結合\n",
    "xs = torch.cat([att_c, prenet_out], dim=1)\n",
    "print(f\"xs = {xs.shape}\")\n",
    "print(\"\")\n",
    "\n",
    "h_list[0], c_list[0] = lstm[0](xs, (h_list[0], c_list[0]))\n",
    "# print(f\"h_list = {h_list} \\nc_list = {c_list}\")\n",
    "\n",
    "for i in range(1, len(lstm)):\n",
    "    h_list[i], c_list[i] = lstm[i](\n",
    "        h_list[i - 1], (h_list[i], c_list[i])\n",
    "    )\n",
    "    # print(f\"h_list = {h_list} \\nc_list = {c_list}\")\n",
    "\n",
    "# 出力の計算\n",
    "hcs = torch.cat([h_list[-1], att_c], dim=1)\n",
    "print(f\"hcs = {hcs.shape}\")\n",
    "\n",
    "outs.append(feat_out(hcs).view(encoder_outs.size(0), out_dim, -1))\n",
    "print(f\"feat_out(hcs) = {feat_out(hcs).shape}\")\n",
    "print(f\"feat_out(hcs)_transposed = {feat_out(hcs).view(encoder_outs.size(0), out_dim, -1).shape}\")\n",
    "# print(f\"outs = {len(outs)}\")\n",
    "\n",
    "logits.append(prob_out(hcs))\n",
    "print(f\"prob_out = {prob_out(hcs).shape}\")\n",
    "\n",
    "att_ws.append(att_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 80])\n",
      "torch.Size([2, 80, 1])\n"
     ]
    }
   ],
   "source": [
    "print(outs[-1][:, :, -1].shape)\n",
    "print(outs[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1, 2, 3], [4, 5, 6]]\n",
    "x[-1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Postnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=80,\n",
    "        layers=5,\n",
    "        channels=512,\n",
    "        kernel_size=5,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        postnet = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            in_channels = in_dim if layer == 0 else channels\n",
    "            out_channels = in_dim if layer == layers - 1 else channels\n",
    "            postnet += [\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=(kernel_size - 1) // 2,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            ]\n",
    "            if layer != layers - 1:\n",
    "                postnet += [nn.Tanh()]\n",
    "            postnet += [nn.Dropout(dropout)]\n",
    "        self.postnet = nn.Sequential(*postnet)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.postnet(xs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Postnet(\n",
       "  (postnet): Sequential(\n",
       "    (0): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Tanh()\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Tanh()\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "    (12): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): Tanh()\n",
       "    (15): Dropout(p=0.5, inplace=False)\n",
       "    (16): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (17): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Postnet()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c46824e6b16a6ff27833fa07a13c1e6ab4fb4daa4a5b0bec23f8bb6c81a58af"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('my_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
